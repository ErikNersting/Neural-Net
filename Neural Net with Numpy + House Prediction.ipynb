{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_size(X,Y, neurons):\n",
    "    \"\"\"\n",
    "    X; Shape (m, X_w)\n",
    "    Y; Shape (m, 1)\n",
    "    \"\"\"\n",
    "    Input_layer = X.shape[1]    #Number of features in X\n",
    "    Hidden_layer = neurons  # number of wanted neurons in hidden layer\n",
    "    Output_Layer = Y.shape[1]  #Number features in Y\n",
    "    \n",
    "    return (Input_layer, Hidden_layer, Output_Layer)  #Return the defined size for each layer in the nn model\n",
    "\n",
    "def init_parameters(Input_layer, Hidden_layer, Output_layer):\n",
    "    \"\"\"This function will initialize weights/bias parameters for the hidden & output layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Weights are initialized as random values, to avoid symmetry. Bias are initialized as 0.\n",
    "    W1 = np.random.randn(Input_layer, Hidden_layer) * 0.01\n",
    "    b1 = np.zeros((1, Hidden_layer))\n",
    "    W2 = np.random.randn(Hidden_layer, Output_layer) * 0.01\n",
    "    b2 = np.zeros((1, Output_layer))\n",
    "    \n",
    "    \n",
    "    #Store values in dictionary for easy access.\n",
    "    parameters = {'W1':W1,\n",
    "                  'b1':b1,\n",
    "                  'W2':W2,\n",
    "                  'b2':b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Function to compute a prediction based on current values of weights & bias.\n",
    "    \"\"\"\n",
    "    #We will use relu as activation function in the hidden layer.\n",
    "    relu = lambda x: np.maximum(x,0) \n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    computation1 = np.dot(x_train, W1) + b1\n",
    "    activation1 = relu(computation1)\n",
    "    computation2 = np.dot(activation1, W2) + b2\n",
    "    activation2 = relu(computation2)\n",
    "    \n",
    "    \n",
    "    cache = {\"computation1\": computation1,\n",
    "             \"activation1\": activation1,\n",
    "             \"computation2\": computation2,\n",
    "             \"activation2\": activation2}\n",
    "    \n",
    "    return activation2, cache\n",
    "    \n",
    "    \n",
    "def cost_function(Y, activation2):\n",
    "    \"\"\"Function to compute cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[0] # number of example\n",
    "\n",
    "    #Mean squared error:\n",
    "    MSE = (activation2 - Y)**2\n",
    "    cost = (1/m)*np.sum(MSE)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def backpropagation(parameters, cache, X, Y):\n",
    "    \"\"\"Compute gradients to optimize network.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    activation1 = cache[\"activation1\"]\n",
    "    activation2 = cache[\"activation2\"]\n",
    "    \n",
    "    \n",
    "     \n",
    "    dZ2 = activation2 - y_train\n",
    "    dW2 = np.dot(dZ2.T, activation1).T / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    dZ1 = np.dot(dZ2, W2.T) * (1 - np.power(activation1, 2))\n",
    "    dW1 = np.dot(dZ1.T, x_train).T / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def parameters_update(parameters, grads, learning_rate, optimizer, beta1, beta2, epsilon, t, L, v, s):\n",
    "    \"\"\"Update the current weights/bias with the chosen learning rate & using backprop function for gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    if optimizer != 'adam':\n",
    "        \n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "\n",
    "        dW1 = grads[\"dW1\"]\n",
    "        db1 = grads[\"db1\"]\n",
    "        dW2 = grads[\"dW2\"]\n",
    "        db2 = grads[\"db2\"]\n",
    "        \n",
    "        W1 = W1 - dW1 * learning_rate\n",
    "        b1 = b1 - db1 * learning_rate\n",
    "        W2 = W2 - dW2 * learning_rate\n",
    "        b2 = b2 - db2 * learning_rate\n",
    "        \n",
    "    elif optimizer == 'adam':\n",
    "\n",
    "        L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "        \n",
    "        v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "        s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "        \n",
    "        # Perform Adam update on all parameters\n",
    "        for l in range(L):\n",
    "            # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "            v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)] + (1 - beta1)*grads['dW' + str(l+1)]\n",
    "            v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)] + (1 - beta1)*grads['db' + str(l+1)]\n",
    "\n",
    "            # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "            v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1 - beta1**t)\n",
    "            v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1 - beta1**t)\n",
    "            \n",
    "            # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "            s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)] + (1 - beta2)*np.square(grads['dW' + str(l+1)])\n",
    "            s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)] + (1 - beta2)*np.square(grads['db' + str(l+1)])\n",
    "\n",
    "            # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "            s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1 - beta2**t)\n",
    "            s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1 - beta2**t)\n",
    "\n",
    "            # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*v_corrected[\"dW\" + str(l+1)]/(np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon)\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*v_corrected[\"db\" + str(l+1)]/(np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon)\n",
    "\n",
    "            return parameters, v, s\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def nn_model(X, Y, n_h, learning_rate, optimizer='adam', epochs = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X; dataset of shape (number of examples, n_features)\n",
    "    Y; labels of shape (number of examples, 1)\n",
    "    n_h; size of the hidden layer\n",
    "    num_iterations; Number of iterations in gradient descent loop\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\"\"\"\n",
    "    \n",
    "    \n",
    "    n_x, n_y = layer_size(X, Y, n_h)[0], layer_size(X, Y, n_h)[2]\n",
    "    parameters = init_parameters(n_x, n_h, n_y)\n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    t=0\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    for l in range(L):\n",
    "    \n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n",
    "    \n",
    "    for i in range(epochs+1):\n",
    "        \n",
    "        #Run forward_prop:\n",
    "        activation2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        #Find cost for current weights:\n",
    "        cost = cost_function(Y, activation2)\n",
    "        \n",
    "        #Run backprop to estimate gradients:\n",
    "        gradients = backpropagation(parameters, cache, X, Y)\n",
    "        \n",
    "        if optimizer != 'adam':\n",
    "            #Update current parameters:\n",
    "            parameters = parameters_update(parameters, gradients, learning_rate, \n",
    "                                                 optimizer, beta1, beta2, epsilon, t, L, v, s)\n",
    "            \n",
    "        elif optimizer == 'adam':\n",
    "            t = t + 1 \n",
    "            parameters, v, s = parameters_update(parameters, gradients, learning_rate, \n",
    "                                                 optimizer, beta1, beta2, epsilon, t, L, v, s)\n",
    "            \n",
    "            \n",
    "        if print_cost and i % 10 == 0:\n",
    "            print(\"Cost after epoch %i: %f\" %(i, cost))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Use forward_prop to predict after model tuning.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    A2 = forward_propagation(X, parameters)[0]\n",
    "\n",
    "    predictions = A2\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Boston Housing Dataset*\n",
    "\n",
    "The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n",
    "\n",
    "CRIM - per capita crime rate by town\n",
    "\n",
    "ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "INDUS - proportion of non-retail business acres per town.\n",
    "\n",
    "CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "\n",
    "NOX - nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "RM - average number of rooms per dwelling\n",
    "\n",
    "AGE - proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "DIS - weighted distances to five Boston employment centres\n",
    "\n",
    "RAD - index of accessibility to radial highways\n",
    "\n",
    "TAX - full-value property-tax rate per $10,000\n",
    "\n",
    "PTRATIO - pupil-teacher ratio by town\n",
    "\n",
    "B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "\n",
    "LSTAT - % lower status of the population\n",
    "\n",
    "MEDV - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "housing = pd.read_csv(\"boston housing.csv\", header=None, delim_whitespace=True, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Let's scale the columns before plotting them against MEDV\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "column_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\n",
    "x = housing.loc[:,column_sels]\n",
    "y = housing['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(housing, y, train_size = 0.6, random_state = 20)\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)\n",
    "y_train =y_train.values.reshape(len(y_train.values),1)\n",
    "y_test = y_test.values.reshape(len(y_test.values),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 606.854970\n",
      "Cost after epoch 10: 83.397058\n",
      "Cost after epoch 20: 83.046159\n",
      "Cost after epoch 30: 82.996189\n",
      "Cost after epoch 40: 82.942895\n",
      "Cost after epoch 50: 82.884869\n",
      "Cost after epoch 60: 82.820762\n",
      "Cost after epoch 70: 82.749097\n",
      "Cost after epoch 80: 82.668221\n",
      "Cost after epoch 90: 82.576255\n",
      "Cost after epoch 100: 82.471125\n",
      "Cost after epoch 110: 82.350533\n",
      "Cost after epoch 120: 82.212067\n",
      "Cost after epoch 130: 82.055399\n",
      "Cost after epoch 140: 81.885626\n",
      "Cost after epoch 150: 81.706236\n",
      "Cost after epoch 160: 81.517228\n",
      "Cost after epoch 170: 81.315527\n",
      "Cost after epoch 180: 81.116824\n",
      "Cost after epoch 190: 80.927141\n",
      "Cost after epoch 200: 80.740675\n"
     ]
    }
   ],
   "source": [
    "model = nn_model(x_train, y_train, 200, 0.001, optimizer='none', epochs = 200, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.84786815974407"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = 0\n",
    "n = 0\n",
    "for i,j in zip(predict(model, x_test), y_test):\n",
    "    MSE = (i - j)**2\n",
    "    cost += np.sum(MSE)\n",
    "    n += 1 \n",
    "cost = cost / n\n",
    "cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
